{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Ensemble Learning\n",
    "\n",
    "* The basic idea of ensemble learning is to have multiple learning algorithms for the same problem and combine their results to make a final prediction\n",
    "\n",
    "* There are multiple types on ensemble learning.  Common approaches include:\n",
    "    * Boosting \n",
    "    * Bagging/Bootstrapping\n",
    "    * Random Forests\n",
    "    * Mixture of Experts\n",
    "    \n",
    "## Bagging\n",
    "\n",
    "* When you have one data set, usually you may train an algorithm and learn a single set of parameters.  However, when we do this, we have no idea how stable/variable those parameters that we estimated are. \n",
    "* Boostrapping can show us the variation in estimated parameter values given a particular data set. Sometimes, it can also help to improve our predictions. \n",
    "* Essentially, to perform bootstrapping, you sample from your data set *with replacement* and train your algorithm to estimate the parameters with each sampled subset.  You can then look at how much the parameters vary with each sampled subset and you can also combine your estimates from each trained method by averaging over all of the results for regression:\n",
    "\\begin{equation}\n",
    "y_{com}(\\mathbf{x}) = \\frac{1}{M} \\sum_{m=1}^M y_m(\\mathbf{x})\n",
    "\\end{equation}\n",
    "* You can aggregate results over all your bootstrap samples using majority vote for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math \n",
    "import textwrap\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def generateRandData(N, l, u, gVar):\n",
    "\t'''generateRandData(N, l, u, gVar): Generate N uniformly random data points in the range [l,u) with zero-mean Gaussian random noise with variance gVar'''\n",
    "\tx = np.random.uniform(l,u,N)\n",
    "\te = np.random.normal(0,gVar,N)\n",
    "\tt = np.sin(2*math.pi*x) + e\n",
    "\treturn x,t\n",
    "\n",
    "def fitdataReg(x,t,M,la):\n",
    "\t'''fitdata(x,t,M): Fit a polynomial of order M to the data (x,t)'''\t\n",
    "\tX = np.array([x**m for m in range(M+1)]).T\n",
    "\tw = np.linalg.inv(X.T@X+(la*np.identity(M+1)))@X.T@t\n",
    "\treturn w\n",
    "\n",
    "def plotPoly(x,t,xrange, y, esty, subplotloc,la=0):\n",
    "\t#plot everything\n",
    "\tplt.subplot(*subplotloc) #identify the subplot to use\n",
    "\t# plt.tight_layout()\n",
    "\tp1 = plt.plot(xrange, y, 'g') #plot true value\n",
    "\tp2 = plt.plot(x, t, 'bo') #plot training data\n",
    "\tp3 = plt.plot(xrange, esty, 'r') #plot estimated value\n",
    "\n",
    "\t#add title, legend and axes labels\n",
    "\tplt.ylabel('t') #label x and y axes\n",
    "\tplt.xlabel('x')\n",
    "\n",
    "def bootstrapRegression(M, numData,percentSample,numSamples):\n",
    "    \n",
    "    #generate data\n",
    "\tx,t = generateRandData(numData,0,1,1) \n",
    "\tnumDataSamples = round(percentSample*numData)\n",
    "\tsubplotloc = [2, round(numSamples/2), 1]\n",
    "\tfig = plt.figure()\n",
    "\txrange = np.arange(0.05,.95,0.001)  #get equally spaced points in the xrange\n",
    "\testy = np.empty([numSamples, xrange.shape[0]])\n",
    "    \n",
    "\tfor iter in range(numSamples):\n",
    "        #select a random subset of the data\n",
    "\t\trp = np.random.permutation(numData)\n",
    "\t\tx_sub = x[rp[0:numDataSamples-1]]\n",
    "\t\tt_sub = t[rp[0:numDataSamples-1]]\n",
    "        \n",
    "        #fit the random subset\n",
    "\t\tw = fitdataReg(x_sub,t_sub,M,0)\n",
    "        \n",
    "        #plot results\n",
    "\t\tsubplotloc[2] = iter+1\n",
    "\t\ty = np.sin(2*math.pi*xrange) #compute the true function value\n",
    "\t\tX = np.array([xrange**m for m in range(w.shape[0])]).T\n",
    "\t\testy[iter,:] = X@w #compute the predicted value\n",
    "\t\tplotPoly(x_sub,t_sub,xrange,y,esty[iter,:],subplotloc)\n",
    "    \n",
    "    #combine the bootstrapped results\n",
    "\tcomy = esty.mean(0)\n",
    "\n",
    "    # compare to full data set\n",
    "\tfig = plt.figure()\n",
    "\tplotPoly(x,t,xrange,y,comy,[1, 1, 1])\n",
    "\n",
    "\tfig = plt.figure()\n",
    "\tw = fitdataReg(x,t,M,0)\n",
    "\ty = np.sin(2*math.pi*xrange) #compute the true function value\n",
    "\tX = np.array([xrange**m for m in range(w.shape[0])]).T\n",
    "\tyy = X@w #compute the predicted value\n",
    "\tplotPoly(x,t,xrange,y,yy, [1, 1, 1])\n",
    "    \n",
    "\n",
    "\n",
    "#Figure 1.7 from text\n",
    "bootstrapRegression(4, 100,.15,20)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting: AdaBoost\n",
    "\n",
    "* Goal:  Combine base (``weak'') classifiers to form a committee whose performance is better than any of the single base classifiers. \n",
    "* The base classifiers are trained in sequence (not in parallel like in bootstrapping)\n",
    "* Each base classifier is trained using a weighted data set (different weights for each base classifier)\n",
    "* Points that are misclassified by a base classifier are weighted more heavily while training the next base classifier\n",
    "\n",
    "* Consider a two class classification problem with $\\mathbf{X} = \\left\\{ \\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\right\\}$ with corresponding labels $y_i \\in \\left\\{ -1,1\\right\\}$.\n",
    "* The goal is to construct a classifier of the form: \n",
    "\\begin{equation}\n",
    "f(\\mathbf{x}) = sign(F(\\mathbf{x}))\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "F(\\mathbf{x}) = \\sum_{k=1}^K \\frac{1}{2}\\alpha_k \\phi(\\mathbf{x}; \\theta_k)\n",
    "\\end{equation}\n",
    "where $\\phi(\\mathbf{x}; \\theta_k)$ is the base classifier. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We need to determine the parameter values for each base classifier:\n",
    "\\begin{eqnarray}\n",
    "\\arg \\min_{\\alpha_k, \\theta_k} \\sum_{i=1}^N \\exp\\left(-y_i F(\\mathbf{x}_i) \\right)\n",
    "\\end{eqnarray}\n",
    "* This cost function penalizes the samples that are incorrectly classified ($y_iF(\\mathbf{x}_i) < 0$) heavily \n",
    "* Direct optimization of all $\\alpha$s and $\\theta$s is difficult.   So, we iteratively optimize (which is sub-optimal). At each stage, we train one base classifier holding fixed all those that have already been trained. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let:\n",
    "\\begin{eqnarray}\n",
    "F_m(\\mathbf{x}) &=& \\sum_{k=1}^m \\frac{1}{2}\\alpha_k \\phi(\\mathbf{x}; \\theta_k)\\\\\n",
    "&=& F_{m-1}(\\mathbf{x}) + \\frac{1}{2}\\alpha_m \\phi(\\mathbf{x}; \\theta_m)\n",
    "\\end{eqnarray}\n",
    "* At step $m$, we optimize for $\\alpha_m$ and $\\theta_m$ where $F_{m-1}(\\mathbf{x})$ is fixed:\n",
    "\\begin{eqnarray}\n",
    "(\\alpha_m, \\theta_m) &=& \\arg \\min_{\\alpha, \\theta} J(\\alpha, \\theta)\\\\\n",
    "&=& \\arg \\min_{\\alpha, \\theta} \\sum_{i=1}^N \\exp\\left( -\\frac{1}{2}y_i\\left( F_{m-1}(\\mathbf{x}_i) + \\alpha\\phi(\\mathbf{x}_i; \\theta)\\right)\\right)\n",
    "\\end{eqnarray}\n",
    "* So, let's optimize this in two steps: first $\\theta_m$ and then $\\alpha_m$\n",
    "\\begin{eqnarray}\n",
    "\\theta_m &=&  \\arg \\min_{\\theta} \\sum_{i=1}^N \\exp\\left( -y_i\\left( F_{m-1}(\\mathbf{x}_i) + \\frac{1}{2}\\alpha\\phi(\\mathbf{x}_i; \\theta)\\right)\\right)\\\\\n",
    "&=& \\arg \\min_{\\theta} \\sum_{i=1}^N w_i^{(m)} \\exp\\left( -\\frac{1}{2}y_i\\alpha\\phi(\\mathbf{x}_i; \\theta)\\right)\n",
    "\\end{eqnarray}\n",
    "where\n",
    "\\begin{equation}\n",
    "w_i^{(m)} = \\exp\\left(-y_iF_{m-1}(\\mathbf{x}_i)\\right)\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This can be re-written as: \n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\theta_m &=&  \\arg \\min_{\\theta} \\exp\\left(-\\alpha_m/2\\right)\\sum_{n \\in T_m}w_n^{(m)} + \\exp\\left(\\alpha_m/2\\right)\\sum_{n \\in M_m}w_n^{(m)} \\nonumber \\\\\n",
    "&=& \\left( \\exp\\left(\\alpha_m/2\\right) - \\exp\\left(-\\alpha_m/2\\right)\\right)\\sum_{i=1}^Nw_i^{(m)} I(\\phi_m(\\mathbf{x}_i;\\theta) \\ne y_i) + \\exp\\left(-\\alpha_m/2\\right)\\sum_{i=1}^Nw_i^{(m)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "* This is equivalent to minimizing\n",
    "\n",
    "\\begin{equation}\n",
    "\\arg \\min_{\\theta} \\sum_{i=1}^N w_i^{(m)} I(\\phi_m(\\mathbf{x}_i;\\theta) \\ne y_i)\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Once we have the optimal classifier at step $m$ (i.e., $\\theta_m$), then we determine the $\\alpha_m$ values\n",
    "\\begin{eqnarray}\n",
    "\\sum_{y_i\\phi(\\mathbf{x}_i;\\theta_m)<0}w_i^{(m)} = P_m\\\\\n",
    "\\sum_{y_i\\phi(\\mathbf{x}_i;\\theta_m)>0}w_i^{(m)} = 1 - P_m\n",
    "\\end{eqnarray}\n",
    "* Plugging this into J, we get:\n",
    "\\begin{eqnarray}\n",
    "\\alpha_m = \\arg\\min_{\\alpha} \\left\\{ \\exp(-\\alpha)(1-P_m) + \\exp(\\alpha)P_m\\right\\}\n",
    "\\end{eqnarray}\n",
    "* Take the derivative with respect to $\\alpha$, set to zero, we get: \n",
    "\\begin{equation}\n",
    "\\alpha_m = \\frac{1}{2}\\ln\\frac{1-P_m}{P_m}\n",
    "\\end{equation}\n",
    "* Once you get $\\theta_m$ and $\\alpha_m$, you compute the weights for the next step:\n",
    "\\begin{equation}\n",
    "w_i^{(m+1)} = \\frac{\\exp(-y_iF_m(\\mathbf{x}_i))}{Z_m} = \\frac{\\exp(-y_i\\alpha_m\\phi(\\mathbf{x}_i;\\theta_m))}{Z_m} \n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "Z_m = \\sum_{i=1}^N w_i^{(m)}\\exp\\left(-y_i\\alpha_m\\phi(\\mathbf{x}_i;\\phi_m)\\right)\n",
    "\\end{equation}\n",
    "* Notice that the weight corresponding to a sample is increased (or decreased) with respect to its value in the previous iteration\n",
    "* Notice that the amount of increase or decrease depends on $\\alpha_m$ which controls the relative importance of the $m^{th}$ term in building up the final classifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "* A forest is made up of many trees...\n",
    "* For classification/regression, put an input vector down each of the trees in the forest.  For classification, classify the data point using majority vote.  For regression, average the values\n",
    "* Each tree is grown using:\n",
    "    * Sample $N$ data points (with replacement, i.e., a bootstrap sample) from the full training data set\n",
    "    * Specify a number $d << D$.  $d$ variables are selected at random out of all $D$ features to determine the split on the node.  Select the best of the $d$ features to split at that node\n",
    "    * Grow each tree as much as possible (i.e., no pruning or stopping early)\n",
    "\n",
    "* Error relates to correlation between the trees.  Greater correlation leads to greater error.  *Does this make sense?*\n",
    "* Error also relates to the strength of each individual tree.  Better individual trees lead to lower error\n",
    "* https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Dropout\n",
    "\n",
    "* This is a method to help prevent overfitting and regularize a network.\n",
    "* The approach attempts to minimize co-dependencies between neurons and enhance robustness of network\n",
    "* Dropout has one parameter $p$.  In each iteration, you randomly exclude each neuron with probability $1-p$ during the training pass (in both forward and backward propagation).  Each iteration, you resample which neurons to keep and which to dropout. \n",
    "* Dropout is related to the concept of ensemble learning with the unique case that the various models in the ensemble share parameters and these models are \"combined\" into a single model/network at test as opposed to training a fusion model or doing a simple average between outputs. \n",
    "* During test, you use all neurons all the time. \n",
    "* Please see and read: https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
